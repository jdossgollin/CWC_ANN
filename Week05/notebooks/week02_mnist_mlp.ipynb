{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST MLP - copied from week 2\n",
    "\n",
    "You should already have gone through the `GettingStartedSequentialModels` notebook -- if not you'll be lost here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use some examples from [https://github.com/fchollet/keras/tree/master/examples](https://github.com/fchollet/keras/tree/master/examples).\n",
    "There are tons more and you should check them out!\n",
    "We'll use these examples to learn about some different sorts of layers, and strategies for our activation functions, loss functions, optimizers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Deep NN on the MNIST Dataset\n",
    "\n",
    "This examples is from [https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py).\n",
    "It's a good one to start with because it's not much more complex than what we have seen, but uses real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist # load up the training data!\n",
    "from keras.models import Sequential # our model\n",
    "from keras.layers import Dense, Dropout # Dropout laters?!\n",
    "from keras.optimizers import RMSprop # our optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically it's good practice to specify your parameters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10 # this is too low "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the data.\n",
    "It's nicely split up between training and testing data which we'll see can be useful.\n",
    "We'll also see that this data treats the images as matrices (row is an observation, column is a pixel).\n",
    "However, the input data _doesn't need to be a matrix_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial then makes a few changes to the data.\n",
    "First, reshape it -- to make sure that the rows and columns are what we expect them to be.\n",
    "Then, divide by 255 so that the values go from 0 to 1.\n",
    "Such scaling is typically a good idea.\n",
    "It also treats the $X$ values as `float32` which you don't have to worry about too much but makes computation a bit faster (at the expense of non-critical numerical detail)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we use the `to_categorical()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax')) # remember y has 10 categories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 171.00 410.00\" width=\"171pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 167,-406 167,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140385258458864 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140385258458864</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 163,-401.5 163,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-379.8\">dense_4_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140385258457968 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140385258457968</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-292.5 30.5,-328.5 132.5,-328.5 132.5,-292.5 30.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-306.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140385258458864&#45;&gt;140385258457968 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140385258458864-&gt;140385258457968</title>\n",
       "<path d=\"M81.5,-365.313C81.5,-357.289 81.5,-347.547 81.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-338.529 81.5,-328.529 78.0001,-338.529 85.0001,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140385258457912 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140385258457912</title>\n",
       "<polygon fill=\"none\" points=\"19,-219.5 19,-255.5 144,-255.5 144,-219.5 19,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-233.8\">dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 140385258457968&#45;&gt;140385258457912 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140385258457968-&gt;140385258457912</title>\n",
       "<path d=\"M81.5,-292.313C81.5,-284.289 81.5,-274.547 81.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-265.529 81.5,-255.529 78.0001,-265.529 85.0001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140385258459088 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140385258459088</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-146.5 30.5,-182.5 132.5,-182.5 132.5,-146.5 30.5,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-160.8\">dense_5: Dense</text>\n",
       "</g>\n",
       "<!-- 140385258457912&#45;&gt;140385258459088 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140385258457912-&gt;140385258459088</title>\n",
       "<path d=\"M81.5,-219.313C81.5,-211.289 81.5,-201.547 81.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-192.529 81.5,-182.529 78.0001,-192.529 85.0001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140385256105968 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140385256105968</title>\n",
       "<polygon fill=\"none\" points=\"19,-73.5 19,-109.5 144,-109.5 144,-73.5 19,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-87.8\">dropout_4: Dropout</text>\n",
       "</g>\n",
       "<!-- 140385258459088&#45;&gt;140385256105968 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140385258459088-&gt;140385256105968</title>\n",
       "<path d=\"M81.5,-146.313C81.5,-138.289 81.5,-128.547 81.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-119.529 81.5,-109.529 78.0001,-119.529 85.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140385256249272 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140385256249272</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-0.5 30.5,-36.5 132.5,-36.5 132.5,-0.5 30.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-14.8\">dense_6: Dense</text>\n",
       "</g>\n",
       "<!-- 140385256105968&#45;&gt;140385256249272 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140385256105968-&gt;140385256249272</title>\n",
       "<path d=\"M81.5,-73.3129C81.5,-65.2895 81.5,-55.5475 81.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-46.5288 81.5,-36.5288 78.0001,-46.5289 85.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment this line if you don't have graphviz installed\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a \"dropout layer\"?\n",
    "See [Quora](https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer):\n",
    "\n",
    "> Using “dropout\", you randomly deactivate certain units (neurons) in a layer with a certain probability $p$. So, if you set half of the activations of a layer to zero, the neural network won’t be able to rely on particular activations in a given feed-forward pass during training. As a consequence, the neural network will learn different, redundant representations; the network can’t rely on the particular neurons and the combination (or interaction) of these to be present. Another nice side effect is that training will be faster.\n",
    "\n",
    "We can use the `summary()` method to look at our model instead of the plot -- this _will_ work on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our model.\n",
    "Note that by giving it a name (`history = model.fit(...`) we'll be able to access some of its outputs.\n",
    "We also use the `validation_data` argument to make it print out the model performance on validation data (which is __not__ used for fitting the model/calculating the back-propagation).\n",
    "The `verbose=1` makes the model talk to us as it fits -- put 0 to make it run silently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2432 - acc: 0.9249 - val_loss: 0.1133 - val_acc: 0.9643\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1017 - acc: 0.9689 - val_loss: 0.1147 - val_acc: 0.9625\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0762 - acc: 0.9772 - val_loss: 0.0922 - val_acc: 0.9744\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0618 - acc: 0.9821 - val_loss: 0.0918 - val_acc: 0.9741\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0506 - acc: 0.9848 - val_loss: 0.0729 - val_acc: 0.9804\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0443 - acc: 0.9865 - val_loss: 0.0834 - val_acc: 0.9794\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0386 - acc: 0.9881 - val_loss: 0.1257 - val_acc: 0.9680\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0366 - acc: 0.9890 - val_loss: 0.0884 - val_acc: 0.9815\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0332 - acc: 0.9903 - val_loss: 0.0865 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0321 - acc: 0.9910 - val_loss: 0.0924 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "finish = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can score our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0923509962029\n",
      "Test accuracy: 0.9805\n",
      "training wall cloc time:  1.74\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('training wall clock time: %5.2f min' % ((finish-start)/60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Validation Performance\n",
    "\n",
    "It's nice to see how our model performs on validation data.\n",
    "This gives us a nice benchmark on how well the model generalizes to data that it hasn't used in training before.\n",
    "However, there are some limitations.\n",
    "\n",
    "- In this case the validation score tells us how well our model does on new MNIST data in the same format as the original data. It doesn't tell us how good it is at image classification on other types of data.\n",
    "- When comparing many models, if we select the model with the best validation score we should be aware that this is a form of overfitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
