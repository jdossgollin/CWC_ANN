\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\title[Your Short Title]{Bayesian Networks}
\author{Adam Massmann}
\institute{Water Center NN Meetings}
\date{Week 5 - Oct. 17th}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Introduction - Information Theory}

\begin{frame}{Introduction - Information Theory}
% \vskip 1cm
\begin{itemize}
\item Consider the ammount of information gained/learned by an event or observation (we'll call $x$). We would receive more new information from a very suprising event than an event we expect.
\item So say we want to quantify the ``ammount of information'' contained in an event. This should then be a function of the probability of the event ($p(x)$). So the ammount of information we'll call a function $h(\cdot )$ will be a function of $p(x)$.
\end{itemize}

\end{frame}


\begin{frame}{Guidance for the functional from of $h(\cdot ) $}
% \vskip 1cm
\begin{itemize}
\item If two events $x$ and $y$ are independent, then the ammount of information gained by both events should be $h(x,y) = h(x) + h(y)$.
\item We also know that the joint probability of $x$ and $y$'s occurence would be: $p(x,y) = p(x) \; p(y)$.
  \item So the question is, what function $h$ satisfies: $h(p(x,y)) = h(p(x)\; p(y)) = h(p(x)) + h(p(y))$?
\end{itemize}

\end{frame}



\begin{frame}{Information Entropy}
% \vskip 1cm
\begin{itemize}
\item $h(\cdot ) = \log ( p(\cdot ))$ satisfies $h(p(x,y)) = h(p(x)\; p(y)) = h(p(x)) + h(p(y))$
\item Its desirable for $h$ to be postiive, so because $0 \leq p \leq 1$, lets make it $h(\cdot ) = -\log p(\cdot )$.
\item Now say we have a bunch of random variables $x$ for which we want to know the average ammount of information (i.e. expectation of $h(x)$). This would be given by:
  \[H[x] = - \sum_x p(x) \log p(x) \]
\item This is known as the \textit{entropy} of $x$.
\item Extending this to continuous variabiles gives the \textit{differential entropy}:
  \[H[x] = - \int p(x)\, \log p(x)\, dx\]
\end{itemize}

\end{frame}

\section{Some \LaTeX{} Examples}

\subsection{Tables and Figures}

\begin{frame}{Tables and Figures}

\begin{itemize}
\item Use \texttt{tabular} for basic tables --- see Table~\ref{tab:widgets}, for example.
\item You can upload a figure (JPEG, PNG or PDF) using the files menu. 
\item To include it in your document, use the \texttt{includegraphics} command (see the comment below in the source code).
\end{itemize}

% Commands to include a figure:
%\begin{figure}
%\includegraphics[width=\textwidth]{your-figure's-file-name}
%\caption{\label{fig:your-figure}Caption goes here.}
%\end{figure}

\begin{table}
\centering
\begin{tabular}{l|r}
Item & Quantity \\\hline
Widgets & 42 \\
Gadgets & 13
\end{tabular}
\caption{\label{tab:widgets}An example table.}
\end{table}

\end{frame}

\subsection{Mathematics}

\begin{frame}{Readable Mathematics}

Let $X_1, X_2, \ldots, X_n$ be a sequence of independent and identically distributed random variables with $\text{E}[X_i] = \mu$ and $\text{Var}[X_i] = \sigma^2 < \infty$, and let
$$S_n = \frac{X_1 + X_2 + \cdots + X_n}{n}
      = \frac{1}{n}\sum_{i}^{n} X_i$$
denote their mean. Then as $n$ approaches infinity, the random variables $\sqrt{n}(S_n - \mu)$ converge in distribution to a normal $\mathcal{N}(0, \sigma^2)$.

\end{frame}

\end{document}
